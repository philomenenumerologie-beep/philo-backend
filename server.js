// server.js
// Backend PhilomÃ¨ne IA
// - MÃ©moire par utilisateur
// - DÃ©compte de tokens
// - GPT-4-Turbo
// - PrÃªt pour Clerk

import express from "express";
import cors from "cors";
import fetch from "node-fetch";

// ============================
// CONFIG
// ============================
const PORT = process.env.PORT || 10000;

// clÃ© OpenAI (tu l'as dÃ©jÃ  mise dans Render > Environment sous OPENAI_API_KEY)
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

// modÃ¨le IA qu'on utilise
const OPENAI_MODEL = "gpt-4-turbo";

// combien de tokens gratuits offerts Ã  un nouveau compte
const START_TOKENS = 5000;

// ============================
// MÃ‰MOIRE EN RAM
// ============================
//
// users[userId] = {
//   tokens: number,
//   history: [ {role:"user"|"assistant"|"system", content:"..."} ]
// }
//
// Note important: Ã§a saute si Render redÃ©marre.
// Pour le moment câ€™est ok pour test.
const users = {};

// assure qu'un user existe dans la mÃ©moire
function ensureUser(userId) {
  if (!users[userId]) {
    users[userId] = {
      tokens: START_TOKENS,
      history: [
        {
          role: "system",
          content:
            "Tu es PhilomÃ¨ne I.A., une assistante personnelle chaleureuse, claire et directe. " +
            "Tu rÃ©ponds en franÃ§ais par dÃ©faut sauf si l'utilisateur parle clairement une autre langue. " +
            "Tu aides pour tout: devoirs, cuisine, pannes techniques (mÃªme avec photo), messages Ã  Ã©crire, administratif, etc. " +
            "Tu donnes des rÃ©ponses utiles, pas trop longues, Ã©tape par Ã©tape si besoin. " +
            "Tu restes polie et rassurante mais tu ne mens pas. " +
            "Pour l'actualitÃ© trÃ¨s rÃ©cente: si tu n'es pas sÃ»re, tu le dis franchement."
        },
        {
          role: "assistant",
          content:
            "Bonjour ðŸ‘‹ Je suis PhilomÃ¨ne I.A.\n" +
            "Tu as un problÃ¨me, jâ€™ai une solution.\n" +
            "Tu as une question, jâ€™ai une rÃ©ponse.\n" +
            "Ensemble on ira plus loin ðŸ•Š"
        }
      ]
    };
  }
  return users[userId];
}

// calcule Ã  peu prÃ¨s les tokens utilisÃ©s pour une rÃ©ponse
// Ici on prend une estimation simple:
//   prompt_length + answer_length â‰ˆ tokens
// Câ€™est pas exact, mais assez bien pour baisser le compteur.
function estimateTokensUsed(promptText, answerText) {
  const promptWords = promptText ? promptText.split(/\s+/).length : 0;
  const answerWords = answerText ? answerText.split(/\s+/).length : 0;
  // on convertit "mots" vers "tokens" (~0.75 ratio en vrai)
  const approx = Math.round((promptWords + answerWords) * 1.3);
  return approx;
}

// ============================
// APP EXPRESS
// ============================
const app = express();

// CORS ouvert pour ton frontend Render
app.use(cors());
app.use(express.json({ limit: "10mb" }));

// ============================
// ROUTE 1: infos utilisateur
// ============================
// frontend va appeler /me au chargement
// en envoyant userId (l'id Clerk)
// On renvoie: tokens restants + historique (pour afficher la discussion)
app.post("/me", (req, res) => {
  const { userId } = req.body || {};

  if (!userId) {
    // pas connectÃ© â†’ on renvoie un profil invitÃ© temporaire
    return res.json({
      guest: true,
      tokens: START_TOKENS, // affichage visuel, mais pas stockÃ©
      history: [
        {
          role: "assistant",
          content:
            "Bonjour ðŸ‘‹ Je suis PhilomÃ¨ne I.A.\n" +
            "Tu as un problÃ¨me, jâ€™ai une solution.\n" +
            "Tu as une question, jâ€™ai une rÃ©ponse.\n" +
            "Ensemble on ira plus loin ðŸ•Š"
        }
      ]
    });
  }

  // user connectÃ©
  const u = ensureUser(userId);

  return res.json({
    guest: false,
    tokens: u.tokens,
    history: u.history.filter(m => m.role !== "system") // on cache le system prompt
  });
});

// ============================
// ROUTE 2: poser une question
// ============================
// le front envoie: { userId, text }
// on renvoie: { answer, tokens }
app.post("/ask", async (req, res) => {
  try {
    const { userId, text } = req.body || {};

    if (!text || text.trim() === "") {
      return res.status(400).json({ error: "Message vide." });
    }

    // si pas de compte â†’ on utilise "guest"
    // guest ne sauvegarde pas l'historique sur plusieurs sessions
    const isGuest = !userId;
    const memoryUserId = isGuest ? "_guest_" : userId;

    const u = ensureUser(memoryUserId);

    // si plus de tokens
    if (u.tokens <= 0) {
      return res.status(402).json({
        error: "Plus assez de tokens.",
        tokens: u.tokens
      });
    }

    // on ajoute le message user dans l'historique
    u.history.push({
      role: "user",
      content: text
    });

    // on limite l'historique pour Ã©viter qu'il explose
    // on garde max ~20 derniers Ã©changes (hors system)
    const systemPart = u.history.filter(m => m.role === "system");
    const convoPart = u.history.filter(m => m.role !== "system");
    if (convoPart.length > 40) {
      // on coupe le dÃ©but
      u.history = [
        ...systemPart,
        ...convoPart.slice(convoPart.length - 40)
      ];
    }

    // on prÃ©pare les messages pour OpenAI
    const messagesForOpenAI = u.history.map(m => ({
      role: m.role,
      content: m.content
    }));

    // appel OpenAI
    let answerText = "DÃ©solÃ©e, je nâ€™ai pas pu rÃ©pondre.";
    if (!OPENAI_API_KEY) {
      // mode test sans clÃ© (sÃ©curitÃ©)
      answerText =
        "Mode test: ajoute la clÃ© OPENAI_API_KEY dans Render pour activer l'IA.";
    } else {
      const r = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${OPENAI_API_KEY}`
        },
        body: JSON.stringify({
          model: OPENAI_MODEL,
          temperature: 0.6,
          messages: messagesForOpenAI
        })
      });

      const data = await r.json();
      if (r.ok && data && data.choices && data.choices[0]) {
        answerText = data.choices[0].message.content.trim();
      } else {
        console.log("OpenAI error:", data);
        answerText =
          "Je nâ€™ai pas rÃ©ussi Ã  contacter lâ€™intelligence. RÃ©essaie dans un instant.";
      }
    }

    // on ajoute la rÃ©ponse de l'assistante dans l'historique
    u.history.push({
      role: "assistant",
      content: answerText
    });

    // on "facture" des tokens
    const usedTokens = estimateTokensUsed(text, answerText);
    u.tokens = Math.max(0, u.tokens - usedTokens);

    return res.json({
      answer: answerText,
      tokens: u.tokens
    });
  } catch (err) {
    console.error("Erreur /ask:", err);
    return res.status(500).json({
      error: "Erreur serveur interne."
    });
  }
});

// ============================
// ROUTE 3: logout (optionnel)
// ============================
// le front pourra juste oublier l'userId cÃ´tÃ© navigateur,
// mais je te donne quand mÃªme un endpoint
app.post("/logout", (req, res) => {
  const { userId } = req.body || {};
  // on ne supprime pas les donnÃ©es pour l'instant
  // comme Ã§a la mÃ©moire reste
  return res.json({ ok: true });
});

// ============================
app.listen(PORT, () => {
  console.log("âœ… PhilomÃ¨ne backend actif sur le port " + PORT);
});
