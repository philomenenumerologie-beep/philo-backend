// server.js
// Backend PhilomÃ¨ne I.A.
// - /ask : conversation texte
// - /analyze-image : analyse d'image
// - mÃ©moire de conversation par utilisateur
// - dÃ©compte des tokens cÃ´tÃ© front (le serveur ne bloque pas encore)
// ------------------------------------------------------------
//
// ATTENTION : tu dois avoir dans package.json :
// {
//   "name": "philomene-backend",
//   "version": "1.0.0",
//   "description": "API PhilomÃ¨ne I.A. avec GPT-5, mÃ©moire persistante et gestion des tokens.",
//   "type": "module",
//   "main": "server.js",
//   "scripts": {
//     "start": "node server.js"
//   },
//   "dependencies": {
//     "express": "^4.19.0",
//     "cors": "^2.8.5",
//     "node-fetch": "^3.3.2",
//     "multer": "^1.4.5-lts.1"
//   }
// }
//
// Et dans Render :
//  - PORT est fourni automatiquement
//  - OPENAI_API_KEY est dÃ©fini dans "Environment Variables"
// ------------------------------------------------------------

import express from "express";
import cors from "cors";
import fetch from "node-fetch";
import multer from "multer";

const app = express();

// ===========================
// CONFIG GÃ‰NÃ‰RALE
// ===========================

// Ta clÃ© OpenAI (doit Ãªtre mise dans Render â†’ Environment â†’ OPENAI_API_KEY)
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || "A_METTRE_DANS_RENDER";

// Choix des modÃ¨les utilisÃ©s
// Texte pur
const OPENAI_MODEL_TEXT = "gpt-4o-mini"; // tu peux plus tard mettre ton modÃ¨le GPT-5 ici
// Vision (analyse d'image)
const OPENAI_MODEL_VISION = "gpt-4o-mini"; // idem

// Limites d'upload
app.use(express.json({ limit: "15mb" }));

// Multer pour rÃ©ceptionner les images envoyÃ©es par le front
const upload = multer({
  storage: multer.memoryStorage(),
  limits: {
    fileSize: 10 * 1024 * 1024 // 10 MB max par image
  }
});

// CORS : autorise seulement ton site
app.use(
  cors({
    origin: [
      "https://philomeneia.com",
      "https://www.philomeneia.com"
    ],
    methods: ["POST", "GET", "OPTIONS"],
    allowedHeaders: ["Content-Type", "Authorization"]
  })
);

// ===========================
// MÃ‰MOIRE DE CONVERSATION
// ===========================
//
// conversations[userId] = [
//   { role:"system", content:"..." },
//   { role:"user", content:"..." },
//   { role:"assistant", content:"..." },
//   ...
// ]
//
// NOTE : c'est en RAM. Donc si Render redÃ©marre, la mÃ©moire repart Ã  zÃ©ro.
// Plus tard on pourra la mettre en base SQLite ou autre.
//
const conversations = {};

function getConversationHistory(userId) {
  if (!conversations[userId]) {
    conversations[userId] = [
      {
        role: "system",
        content:
          "Tu es PhilomÃ¨ne I.A., une assistante personnelle franÃ§aise. " +
          "Tu rÃ©ponds clairement, simplement, sans blabla inutile. " +
          "Tu peux Ãªtre sympa et directe. " +
          "Tu donnes des infos concrÃ¨tes et pratiques. " +
          "Tu restes polie et tu Ã©vites les phrases trop longues."
      }
    ];
  }
  return conversations[userId];
}

function pushToConversation(userId, role, content) {
  const history = getConversationHistory(userId);
  history.push({ role, content });

  // On limite la taille mÃ©moire par utilisateur pour Ã©viter que Ã§a explose.
  // On garde le message system + les ~30 derniers tours.
  const MAX_MESSAGES = 60; // total (system + Ã©changes)
  if (history.length > MAX_MESSAGES) {
    const systemMsg = history[0]; // on garde la premiÃ¨re consigne
    const lastMsgs = history.slice(-MAX_MESSAGES + 1);
    conversations[userId] = [systemMsg, ...lastMsgs];
  }
}

// ===========================
// APPEL OPENAI (TEXTE)
// ===========================
//
// On envoie l'historique complet du user Ã  OpenAI.
// IMPORTANT : pas de "temperature" custom ici car le modÃ¨le actuel
// n'accepte pas de valeur diffÃ©rente de la valeur par dÃ©faut.
// (c'Ã©tait ton erreur 'Unsupported value: temperature')
//
async function askOpenAIText(messages) {
  const body = {
    model: OPENAI_MODEL_TEXT,
    messages
    // PAS de "temperature": 0.7
  };

  const resp = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${OPENAI_API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify(body)
  });

  if (!resp.ok) {
    const textErr = await resp.text();
    console.error("âŒ OpenAI /text status:", resp.status);
    console.error("âŒ OpenAI /text body:", textErr);
    throw new Error("Erreur API OpenAI (texte)");
  }

  const data = await resp.json();

  const answer =
    data?.choices?.[0]?.message?.content?.trim() ||
    "Je suis dÃ©solÃ©e, je n'ai pas pu gÃ©nÃ©rer de rÃ©ponse.";

  return answer;
}

// ===========================
// APPEL OPENAI (VISION / IMAGE)
// ===========================
//
// On fabrique un message 'user' qui contient :
// - du texte (la question de l'utilisateur genre 'DÃ©cris moi la machine')
// - l'image encodÃ©e en base64 sous forme d'URL data:...
//
// Pareil : PAS de 'temperature' custom.
//
async function askOpenAIVision({ question, dataUrl }) {
  const messages = [
    {
      role: "system",
      content:
        "Tu es PhilomÃ¨ne I.A., assistante franÃ§aise. " +
        "Tu regardes l'image fournie par l'utilisateur et tu expliques clairement " +
        "ce qu'il y a dessus. Si tu n'es pas sÃ»re, tu le dis."
    },
    {
      role: "user",
      content: [
        {
          type: "text",
          text:
            question ||
            "Analyse l'image. Dis-moi ce que tu vois et Ã  quoi Ã§a sert."
        },
        {
          type: "image_url",
          image_url: dataUrl
        }
      ]
    }
  ];

  const body = {
    model: OPENAI_MODEL_VISION,
    messages
    // PAS de "temperature"
  };

  const resp = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${OPENAI_API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify(body)
  });

  if (!resp.ok) {
    const textErr = await resp.text();
    console.error("âŒ OpenAI /vision status:", resp.status);
    console.error("âŒ OpenAI /vision body:", textErr);
    throw new Error("Erreur API OpenAI (vision)");
  }

  const data = await resp.json();

  const answer =
    data?.choices?.[0]?.message?.content?.trim() ||
    "J'ai bien reÃ§u l'image mais je n'ai pas pu l'analyser.";

  return answer;
}

// ===========================
// ROUTE /ask
// ===========================
//
// Le front envoie :
// {
//   conversation: [...],  // historique local (on ne fait plus confiance 100%, on prend juste le dernier user message)
//   userId: "guest" OU un vrai id,
//   tokens: 980            // le solde estimÃ© cÃ´tÃ© front (info facultative)
// }
//
// Le backend :
// 1. rÃ©cupÃ¨re le dernier message user
// 2. l'ajoute dans la mÃ©moire du serveur
// 3. envoie toute la mÃ©moire user -> OpenAI
// 4. ajoute la rÃ©ponse en mÃ©moire
// 5. renvoie { answer, tokensLeft }
app.post("/ask", async (req, res) => {
  try {
    const { conversation, userId, tokens } = req.body || {};
    const uid = userId || "guest";

    // On chope le dernier message utilisateur depuis ce que le front nous a envoyÃ©.
    let lastUserMessage = null;
    if (Array.isArray(conversation)) {
      for (let i = conversation.length - 1; i >= 0; i--) {
        const msg = conversation[i];
        if (msg.role === "user" && msg.content && msg.content.trim()) {
          lastUserMessage = msg.content.trim();
          break;
        }
      }
    }

    if (!lastUserMessage) {
      return res.status(400).json({
        error: "Pas de message utilisateur reÃ§u."
      });
    }

    // On stocke le dernier message utilisateur dans la mÃ©moire backend
    pushToConversation(uid, "user", lastUserMessage);

    // On rÃ©cupÃ¨re l'historique complet (system + tout)
    const fullHistory = getConversationHistory(uid);

    // On demande la rÃ©ponse Ã  OpenAI avec tout l'historique
    const answer = await askOpenAIText(fullHistory);

    // On stocke aussi la rÃ©ponse dans la mÃ©moire
    pushToConversation(uid, "assistant", answer);

    // On renvoie la rÃ©ponse
    // tokensLeft : pour l'instant on renvoie ce que le front nous a dit.
    // (le vrai blocage de tokens se fera plus tard cÃ´tÃ© serveur si tu veux)
    res.json({
      answer,
      tokensLeft: tokens
    });
  } catch (err) {
    console.error("ğŸ”¥ Erreur /ask:", err);
    return res.status(500).json({
      error: "Erreur interne /ask."
    });
  }
});

// ===========================
// ROUTE /analyze-image
// ===========================
//
// Form-data attendu (multipart/form-data) :
//   - "image": le fichier (photo, screenshot, etc.)
//   - "userId": identifiant user ou "guest"
//   - "prompt": texte optionnel ("Qu'est-ce que c'est cette machine ?")
//
// Ã‰tapes :
// 1. on convertit l'image reÃ§ue en base64 -> data URL
// 2. on prÃ©pare la question
// 3. on envoie Ã  askOpenAIVision()
// 4. on sauvegarde question/rÃ©ponse dans la mÃ©moire
//
app.post("/analyze-image", upload.single("image"), async (req, res) => {
  try {
    const uid = req.body?.userId || "guest";
    const userPrompt =
      req.body?.prompt ||
      "DÃ©cris-moi prÃ©cisÃ©ment l'image et dis-moi Ã  quoi elle sert.";

    if (!req.file) {
      return res.status(400).json({ error: "Aucune image reÃ§ue." });
    }

    // On convertit le binaire reÃ§u en base64 + data URL
    const mimeType = req.file.mimetype || "image/jpeg";
    const base64 = req.file.buffer.toString("base64");
    const dataUrl = `data:${mimeType};base64,${base64}`;

    // On push dans la mÃ©moire du user : il a demandÃ© une analyse d'image
    pushToConversation(
      uid,
      "user",
      `${userPrompt} [image envoyÃ©e]`
    );

    // On interroge le modÃ¨le vision
    const visionAnswer = await askOpenAIVision({
      question: userPrompt,
      dataUrl
    });

    // On stocke la rÃ©ponse dans la mÃ©moire
    pushToConversation(uid, "assistant", visionAnswer);

    // On renvoie la rÃ©ponse vision
    res.json({
      answer: visionAnswer
    });
  } catch (err) {
    console.error("ğŸ”¥ Erreur /analyze-image:", err);
    return res.status(500).json({
      error: "Erreur interne /analyze-image."
    });
  }
});

// ===========================
// HEALTHCHECK /
// ===========================
app.get("/", (_req, res) => {
  res.send("âœ… API PhilomÃ¨ne I.A. en ligne (GPT-5, mÃ©moire persistante, tokens rÃ©els).");
});

// ===========================
// LANCEMENT SERVEUR
// ===========================
//
// Render va donner PORT dans les vars d'env.
// En local tu peux faire `PORT=10000 node server.js`
const PORT = process.env.PORT || 10000;
app.listen(PORT, () => {
  console.log("ğŸš€ PhilomÃ¨ne backend dÃ©marrÃ© sur le port " + PORT);
});
